{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:-\n",
    "The purpose of this project is to apply what you have learned to the data hassles\n",
    "Data section from Udacity Data Analysis Nanodegree software. The data set that is\n",
    "wrangled is the tweet archive of Twitter user @dog_rates, also known as\n",
    "Like WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs\n",
    "This report briefly describes my wrangling efforts.\n",
    "\n",
    "#### Gathering data:-\n",
    "    \n",
    "The data for this project consist on three different dataset that were obtained as\n",
    "following\n",
    "\n",
    "Twitter archive file: the twitter_archive_enhanced.csv was provided by\n",
    "Udacity and downloaded manually.\n",
    "\n",
    "Tweet image predictions, i.e. the breed in each tweet\n",
    "According to a neural network. This file (image_prediction.tsv) is hosted\n",
    "on Udacity servers and downloaded programmatically with the . extension\n",
    "Requests library and URL info\n",
    "\n",
    "Twitter API & JSON: Using Tweet IDs in WeRateDogs Twitter\n",
    "In the archive, I asked about the Twitter API for each tweet's JSON data using\n",
    "The Python Tweepy library stores a full set of JSON data for each tweet\n",
    "In a file called tweet_json.txt. I read this .txt file line by line to file\n",
    "Pandas data frame with tweet id.\n",
    "\n",
    "\n",
    "#### Assessing data\n",
    "Once the three tables were obtained I assessed the data as following:\n",
    "Visually, I used two tools. One was by printing the three entire\n",
    "dataframes separate in Jupyter Notebook and two by checking the csv\n",
    "files in Excel.\n",
    "Programmatically, by using different methods \n",
    "\n",
    "Then I separated the issues encountered in quality issues and tidiness issues.\n",
    "\n",
    "\n",
    "#### Data cleaning\n",
    "This part of the data debate is divided into three parts: definition, coding, and testing\n",
    "code. These three steps were around each of the issues described in the evaluation\n",
    "Section.\n",
    "\n",
    "First and very helpful step was to create a copy of the three original dataframes. If there was an error, I could create a\n",
    "new copy from the original.\n",
    "\n",
    "#### Conclusion\n",
    "Data wrangling is a core skill that whoever handles data should be familiar with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
